The linear search algorithm was implemented and its performance was analyzed in three scenarios: best case (element at the beginning), average case (element in the middle), and worst case
(element not present). A list of random integers was generated for different input sizes (5000 to 20000), and the time taken to search a key in each case was measured over 100 trials. In the best case,
the algorithm performs in constant time O(1) since it finds the element at index 0. In the average case, it performs in O(n/2), as it typically finds the element midway through the list. In the worst 
case, where the element is absent, the entire list is traversed, giving it a time complexity of O(n). The results were plotted with input size on the X-axis and average search time on the Y-axis, 
showing a near-constant time for the best case and a linear increase for average and worst cases. Execution times confirmed that as the input size increases, the worst-case performance degrades 
significantly, while the best case remains almost unaffected. This demonstrates that while linear search is simple and works well for small or unsorted datasets, it becomes inefficient for large 
inputs, making optimized algorithms like binary search or hashing preferable in such scenarios.
